{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b022305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading module 'cuda/12.2.0'\u001b[m\n",
      "Loading module 'gcc/9.2.0'\u001b[m\n",
      "\u001b[m\n",
      "Loading \u001b[1mcuda/12.2.0\u001b[22m\u001b[m\n",
      "  \u001b[94mLoading requirement\u001b[0m: gcc/9.2.0\u001b[m\n",
      "\u001b[K\u001b[?1l\u001b>Sat Dec 14 20:59:32 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          Off | 00000000:E2:00.0 Off |                    0 |\n",
      "| N/A   32C    P0              63W / 300W |  31512MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A   1109356      C   ...097/.conda/envs/final_ml/bin/python    15778MiB |\n",
      "|    0   N/A  N/A   1135894      C   ...097/.conda/envs/final_ml/bin/python    15716MiB |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "/bin/bash: nvcc: command not found\n",
      "# conda environments:\n",
      "#\n",
      "final_ml                 /home/rk4097/.conda/envs/final_ml\n",
      "qiime2-2023.2            /home/rk4097/.conda/envs/qiime2-2023.2\n",
      "                         /home/rk4097/miniconda3\n",
      "                         /home/rk4097/miniconda3/envs/qiime2-2023.2\n",
      "base                  *  /share/apps/NYUAD5/miniconda/3-4.11.0/envs/jupyter\n",
      "\n",
      "Environment: /home/rk4097/.conda/envs/final_ml/bin/python\n",
      "2.5.1\n",
      "12.4\n",
      "Is GPU available: True\n"
     ]
    }
   ],
   "source": [
    "!module load cuda/12.2.0\n",
    "!nvidia-smi\n",
    "\n",
    "!nvcc --version\n",
    "!conda info --envs\n",
    "\n",
    "import sys\n",
    "print(\"Environment:\", sys.executable)\n",
    "\n",
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(\"Is GPU available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2c45a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/williamscott701/memotion-dataset-7k\n",
      "License(s): other\n",
      "memotion-dataset-7k.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
      "Dataset extracted to 'memotion-dataset/'\n",
      "Dataset successfully prepared in 'memotion-dataset'.\n",
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# General\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Kaggle API configuration\n",
    "kaggle_json_path = \"kaggle.json\"\n",
    "if not os.path.exists(kaggle_json_path):\n",
    "    raise FileNotFoundError(\"kaggle.json not found in the current directory.\")\n",
    "\n",
    "# Create the ~/.kaggle directory if it doesn't exist\n",
    "os.makedirs(os.path.expanduser(\"~/.kaggle\"), exist_ok=True)\n",
    "\n",
    "# Move kaggle.json to the ~/.kaggle directory\n",
    "os.system(f\"cp {kaggle_json_path} ~/.kaggle/\")\n",
    "os.system(\"chmod 600 ~/.kaggle/kaggle.json\")\n",
    "\n",
    "# Specify the dataset name\n",
    "dataset_name = \"williamscott701/memotion-dataset-7k\"\n",
    "\n",
    "# Add Kaggle CLI to PATH if necessary\n",
    "kaggle_path = os.path.expanduser(\"~/.conda/envs/final_ml/bin/kaggle\")\n",
    "os.environ[\"PATH\"] += os.pathsep + os.path.dirname(kaggle_path)\n",
    "\n",
    "# Verify Kaggle CLI installation\n",
    "kaggle_version = os.popen(f\"{kaggle_path} --version\").read()\n",
    "if \"Kaggle API\" not in kaggle_version:\n",
    "    raise EnvironmentError(\"Kaggle CLI is not properly installed or accessible.\")\n",
    "\n",
    "# Download the dataset\n",
    "download_status = os.system(f\"{kaggle_path} datasets download -d {dataset_name}\")\n",
    "if download_status != 0:\n",
    "    raise RuntimeError(\"Failed to download dataset using Kaggle CLI.\")\n",
    "\n",
    "# Unzip the dataset\n",
    "zip_file = \"memotion-dataset-7k.zip\"\n",
    "if os.path.exists(zip_file):\n",
    "    with zipfile.ZipFile(zip_file, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(\"memotion-dataset\")\n",
    "    print(\"Dataset extracted to 'memotion-dataset/'\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"{zip_file} not found after Kaggle download.\")\n",
    "\n",
    "# Verify dataset contents\n",
    "dataset_folder = \"memotion-dataset\"\n",
    "if not os.path.exists(dataset_folder):\n",
    "    raise FileNotFoundError(\"Dataset folder not found after extraction.\")\n",
    "else:\n",
    "    print(f\"Dataset successfully prepared in '{dataset_folder}'.\")\n",
    "\n",
    "print(\"Setup complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54c7d05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0    image_name  \\\n",
      "0           0   image_1.jpg   \n",
      "1           1  image_2.jpeg   \n",
      "2           2   image_3.JPG   \n",
      "3           3   image_4.png   \n",
      "4           4   image_5.png   \n",
      "\n",
      "                                            text_ocr  \\\n",
      "0  LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...   \n",
      "1  The best of #10 YearChallenge! Completed in le...   \n",
      "2  Sam Thorne @Strippin ( Follow Follow Saw every...   \n",
      "3              10 Year Challenge - Sweet Dee Edition   \n",
      "4  10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...   \n",
      "\n",
      "                                      text_corrected      humour  \\\n",
      "0  LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...   hilarious   \n",
      "1  The best of #10 YearChallenge! Completed in le...   not_funny   \n",
      "2  Sam Thorne @Strippin ( Follow Follow Saw every...  very_funny   \n",
      "3              10 Year Challenge - Sweet Dee Edition  very_funny   \n",
      "4  10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...   hilarious   \n",
      "\n",
      "           sarcasm       offensive      motivational overall_sentiment  \n",
      "0          general   not_offensive  not_motivational     very_positive  \n",
      "1          general   not_offensive      motivational     very_positive  \n",
      "2    not_sarcastic   not_offensive  not_motivational          positive  \n",
      "3  twisted_meaning  very_offensive      motivational          positive  \n",
      "4     very_twisted  very_offensive  not_motivational           neutral  \n",
      "0    2\n",
      "1    0\n",
      "2    2\n",
      "3    2\n",
      "4    2\n",
      "Name: humour, dtype: int64\n",
      "(6992,)\n"
     ]
    }
   ],
   "source": [
    "unprocessed_data = pd.read_csv('memotion-dataset/memotion_dataset_7k/labels.csv')\n",
    "# Verify that the file is loaded properly\n",
    "print(unprocessed_data.head())\n",
    "\n",
    "# # Select the desired columns: the text in the image, and the label for offensivness\n",
    "# processed_data = unprocessed_data[\"offensive\"]\n",
    "\n",
    "# # Map the labels in the column \"offensive\"\n",
    "# labels_map = {\"not_offensive\":0, \"slight\":1, \"very_offensive\":2, \"hateful_offensive\":2}\n",
    "# processed_data = processed_data.map(labels_map)\n",
    "\n",
    "\n",
    "# Select the desired columns: the text in the image, and the label for offensivness\n",
    "processed_data = unprocessed_data[\"humour\"]\n",
    "\n",
    "# Map the labels in the column \"humour\"\n",
    "labels_map = {\"funny\":1, \"very_funny\":2, \"not_funny\":0, \"hilarious\":2}\n",
    "processed_data = processed_data.map(labels_map)\n",
    "\n",
    "# Verify by printing the first few values and by checking the number of rows\n",
    "print(processed_data.head())\n",
    "print(processed_data.shape) # should be equal to number of images\n",
    "\n",
    "# Save the processed data to avoid running this cell multiple times\n",
    "processed_data.to_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8625a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efab5f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6992\n",
      "Number of valid texts: 6986\n",
      "Number of valid targets: 6986\n",
      "Indices of disregarded images: [119, 4799, 5118, 6781, 6784, 6786]\n",
      "13035\n",
      "One-hot vector for '207r': [0. 0. 0. ... 0. 0. 0.] has shape (13035,)\n",
      "memotion-dataset/memotion_dataset_7k/images/image_120.jpg\n",
      "memotion-dataset/memotion_dataset_7k/images/image_4800.jpg\n",
      "memotion-dataset/memotion_dataset_7k/images/image_5119.png\n",
      "memotion-dataset/memotion_dataset_7k/images/image_6782.jpg\n",
      "memotion-dataset/memotion_dataset_7k/images/image_6785.jpg\n",
      "memotion-dataset/memotion_dataset_7k/images/image_6787.jpg\n",
      "6986 6986\n"
     ]
    }
   ],
   "source": [
    "# Extract the texts\n",
    "texts = unprocessed_data['text_corrected'].tolist()\n",
    "print(len(texts))\n",
    "\n",
    "import re\n",
    "\n",
    "# normalize input text\n",
    "def normalize(text):\n",
    "    # Lowercase and split text into words\n",
    "    #return [word.lower().strip('.,!?') for word in text.split()]\n",
    "    return re.findall(r\"\\w+|[^\\w\\s]\", text.lower())\n",
    "\n",
    "# Extract the targets\n",
    "labels = processed_data.tolist()  # Convert to list for easier indexing\n",
    "\n",
    "# Find indices of empty or invalid texts\n",
    "disregarded_indices = [i for i, text in enumerate(texts) if not isinstance(text, str) or not text.strip() or i == 5118]\n",
    "\n",
    "# Remove texts and corresponding targets\n",
    "texts = [text for i, text in enumerate(texts) if i not in disregarded_indices]\n",
    "labels = [label for i, label in enumerate(labels) if i not in disregarded_indices]\n",
    "\n",
    "print(f\"Number of valid texts: {len(texts)}\")\n",
    "print(f\"Number of valid targets: {len(labels)}\")\n",
    "print(f\"Indices of disregarded images: {disregarded_indices}\")\n",
    "\n",
    "# Split texts into words\n",
    "words = [word for text in texts if isinstance(text, str) and text.strip() for word in normalize(text)] ## From ChatGPT\n",
    "\n",
    "# Create vocabulary of unique words\n",
    "vocabulary = sorted(set(words))\n",
    "word_to_index = {word: idx for idx, word in enumerate(vocabulary)}  ### From ChatGPT\n",
    "\n",
    "# Create 1-hot vectors for each word in the vocabulary\n",
    "vocab_size = len(vocabulary)\n",
    "print(vocab_size)\n",
    "one_hot_vectors = np.eye(vocab_size)  # (identity matrix with size of vocab)\n",
    "\n",
    "# Save to a matrix with word-to-one-hot mapping\n",
    "one_hot_matrix = {word: one_hot_vectors[idx] for word, idx in word_to_index.items()}\n",
    "\n",
    "# Verify\n",
    "word = vocabulary[200]\n",
    "one_hot_vector = one_hot_matrix[word]\n",
    "print(f\"One-hot vector for '{word}': {one_hot_vector}\", \"has shape\", one_hot_vector.shape)\n",
    "\n",
    "# Imports for Image Processing\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define image transformations\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 128x128\n",
    "    transforms.ToTensor()          # Convert PIL Image to Tensor\n",
    "])\n",
    "\n",
    "images_dir = 'memotion-dataset/memotion_dataset_7k/images' \n",
    "image_files = sorted(os.listdir(images_dir)) #ensuring alignment with texts\n",
    "\n",
    "image_paths = []\n",
    "for path in image_files:\n",
    "    pattern = r\"_(\\d+)\\.\"\n",
    "    idx = int(re.search(pattern, path).group(1))\n",
    "    if idx - 1 not in disregarded_indices:\n",
    "        image_paths.append(os.path.join(images_dir, path))\n",
    "    else:\n",
    "        print(os.path.join(images_dir, path))\n",
    "\n",
    "## sort the image based on the number NOT string\n",
    "image_paths = sorted(image_paths, key=lambda path: int(re.search(pattern, path).group(1)))\n",
    "print(len(image_paths), len(texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a38b3d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images: 5588, Validation images: 699, Test images: 699\n",
      "memotion-dataset/memotion_dataset_7k/images/image_100.jpg Drink water you may not meme-generator.com 2\n"
     ]
    }
   ],
   "source": [
    "# Some of the codes here are from Assignment 5\n",
    "\n",
    "n = int(0.8 * len(labels)) # n is used to split train(80%)/test+val(20%)\n",
    "m = int(0.9 * len(labels)) # m is used to split test(10%)/val(10%)\n",
    "\n",
    "train_labels = labels[:n]\n",
    "val_labels = labels[n:m]\n",
    "test_labels = labels[m:]\n",
    "\n",
    "train_texts = texts[:n]\n",
    "val_texts = texts[n:m]\n",
    "test_texts = texts[m:]\n",
    "\n",
    "\n",
    "# Split image paths corresponding to text splits\n",
    "train_image_paths = image_paths[:n]\n",
    "val_image_paths = image_paths[n:m]\n",
    "test_image_paths = image_paths[m:]\n",
    "\n",
    "print(f\"Train images: {len(train_image_paths)}, Validation images: {len(val_image_paths)}, Test images: {len(test_image_paths)}\")\n",
    "\n",
    "print(image_paths[99], texts[99], labels[99])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0acce76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edb68b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# transformer hyperparams (some taken from Assignment 5)\n",
    "n_classes = 3 # Number of classes in the classification process\n",
    "batch_size = 128 # 64 # batch size\n",
    "block_size = 128 # context length\n",
    "# block_size = 256 # context length\n",
    "window_stride = 5 # 2 # number of words to skip in the sliding window when the text is > block_size\n",
    "max_iters = 5000 # number of training epochs\n",
    "eval_interval = 500 # set to evaluation mode to compute losses every `eval_interval` epochs\n",
    "learning_rate = 3e-4 # learning rate for optimization\n",
    "eval_iters = 200 # 200 # number of iteration when in evaluation mode\n",
    "hotvector_dim = vocab_size # Hot vector size\n",
    "n_embd = 384 # 768 # embedding dimension\n",
    "n_head = 2 # 2 # number of heads\n",
    "n_layer = 2 # 2 # number of layers\n",
    "dropout = 0.2 # dropout rate\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48658139",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnModel(nn.Module):\n",
    "    def __init__(self, feature_dim=256):\n",
    "        super(CnnModel, self).__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # Input is 3 * 224 * 224\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Output: 32 x 112 x 112\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Output: 64 x 56 x 56\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Output: 128 x 28 x 28\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Output: 256 x 14 x 14\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),                       # Flatten the tensor\n",
    "            nn.Linear(256 * 14 * 14, 1024),       # Fully connected layer\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, n_embd),  # Final feature vector\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)  # Pass through convolutional layers\n",
    "        x = self.fc(x)           # Pass through fully connected layers\n",
    "        return x # Reduced to feature_dim; (B, feature_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af09c738",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e6ea851",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "def get_batch_with_images(split):\n",
    "    # Fetch data based on split\n",
    "    if split == 'train':\n",
    "        data = train_texts\n",
    "        targets = train_labels\n",
    "        img_paths = train_image_paths\n",
    "    elif split == 'val':\n",
    "        data = val_texts\n",
    "        targets = val_labels\n",
    "        img_paths = val_image_paths\n",
    "    elif split == 'test':\n",
    "        data = test_texts\n",
    "        targets = test_labels\n",
    "        img_paths = test_image_paths\n",
    "\n",
    "    # Generate random indices for batch\n",
    "    ix = torch.randint(0, len(data), (batch_size,))\n",
    "\n",
    "    x_text, x_images, y, masks, window_map = [], [], [], [], []\n",
    "    current_window_id = 0\n",
    "    n_batches = 0\n",
    "\n",
    "    for i in ix:\n",
    "        if n_batches >= batch_size:\n",
    "            break\n",
    "            \n",
    "        input_text = data[i]\n",
    "        target_label = targets[i]\n",
    "\n",
    "        # Process text\n",
    "        one_hot_input = torch.stack([torch.tensor(one_hot_matrix[word]) for word in normalize(input_text) if word in one_hot_matrix])\n",
    "\n",
    "        if one_hot_input.shape[0] > block_size:\n",
    "            # Split into sliding windows\n",
    "            \n",
    "#             print(\"Text size bigger than block_size was found\")\n",
    "            windows = sliding_window_split(input_text, block_size, window_stride)\n",
    "            for window in windows:\n",
    "                if n_batches >= batch_size:\n",
    "                     break\n",
    "\n",
    "                one_hot_window = torch.stack([torch.tensor(one_hot_matrix[word]) for word in window if word in one_hot_matrix])\n",
    "                mask = torch.ones(block_size)\n",
    "\n",
    "                # Pad if the window is smaller than block_size\n",
    "                if one_hot_window.shape[0] < block_size:\n",
    "                    padding = torch.zeros(block_size - one_hot_window.shape[0], vocab_size)\n",
    "                    one_hot_window = torch.cat((one_hot_window, padding), dim=0)\n",
    "                    mask = torch.cat((torch.ones(one_hot_window.shape[0]), torch.zeros(block_size - one_hot_window.shape[0])))\n",
    "\n",
    "                x_text.append(one_hot_window)\n",
    "                masks.append(mask)\n",
    "                window_map.append(current_window_id)\n",
    "                n_batches += 1\n",
    "            \n",
    "            y.append(target_label)\n",
    "            current_window_id += 1\n",
    "\n",
    "            # Process image\n",
    "            image_path = img_paths[i]\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image = image_transforms(image).to(device)\n",
    "            x_images.append(image)\n",
    "\n",
    "\n",
    "        elif one_hot_input.shape[0] < block_size:\n",
    "            padding = torch.zeros(block_size - one_hot_input.shape[0], vocab_size)\n",
    "            one_hot_input = torch.cat((one_hot_input, padding), dim=0)\n",
    "            mask = torch.cat((torch.ones(one_hot_input.shape[0]), torch.zeros(block_size - one_hot_input.shape[0])))\n",
    "            x_text.append(one_hot_input)\n",
    "            masks.append(mask)\n",
    "            window_map.append(current_window_id)\n",
    "            y.append(target_label)\n",
    "            current_window_id += 1\n",
    "            n_batches += 1\n",
    "            \n",
    "             # Process image\n",
    "            image_path = img_paths[i]\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image = image_transforms(image).to(device)\n",
    "            x_images.append(image)\n",
    "\n",
    "            \n",
    "        else:\n",
    "            # Exact block size\n",
    "            mask = torch.ones(block_size)\n",
    "            x_text.append(one_hot_input)\n",
    "            masks.append(mask)\n",
    "            window_map.append(current_window_id)\n",
    "            y.append(target_label)\n",
    "            current_window_id += 1\n",
    "            n_batches += 1\n",
    "            \n",
    "             # Process image\n",
    "            image_path = img_paths[i]\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image = image_transforms(image).to(device)\n",
    "            x_images.append(image)\n",
    "            \n",
    "    x_text = torch.stack(x_text).to(device, dtype=torch.float)  # (B, block_size, vocab_size)\n",
    "    x_images = torch.stack(x_images).to(device, dtype=torch.float)  # (B, 3, 128, 128)\n",
    "    y = torch.tensor(y, dtype=torch.long).to(device)\n",
    "    masks = torch.stack(masks).to(device, dtype=torch.float)\n",
    "\n",
    "    return x_text, x_images, y, masks, window_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d9dbe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_split(input_text, block_size, window_stride):\n",
    "    tokens = input_text.split()\n",
    "    windows = []\n",
    "    for start in range(0, len(tokens) - block_size + 1, window_stride):\n",
    "        window = tokens[start:start + block_size]\n",
    "        windows.append(window)\n",
    "    # Add the last window if it's incomplete\n",
    "    if len(tokens) % block_size != 0: ## This part is from ChatGPT\n",
    "        windows.append(tokens[-block_size:])\n",
    "    return windows\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" Single Head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        # create keys, queries, values using a single linear layer with input size = n_embd, and output size = head_size. Set bias=False!\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout) # dropout layer\n",
    "\n",
    "    ### Mask is used to remove only Padding while doing self attention\n",
    "    def forward(self, x, mask):\n",
    "        B,T,C = x.shape # recover the shape of input x\n",
    "        # pass raw input x through the key and query layers\n",
    "        k = self.key(x)   # (B, T, head_size)\n",
    "        q = self.query(x) # (B, T, head_size)\n",
    "        # perform self-attention between keys and queries\n",
    "        wei = q @ k.transpose(-2, -1) # (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
    "        # Mask paddings before doing self attention\n",
    "        wei = wei.masked_fill(mask.unsqueeze(1) == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T) # normalization by softmax\n",
    "        wei = self.dropout(wei) # sets a fraction (determined by the hyperparam dropout) of its elements to zero during training, thus providing a form of regularization\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B, T, head_size)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, head_size) -> (B, T, head_size)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        # Attention Mechanism (from Assignment 5)\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)]) # concatenating multiple heads in parallel in a module\n",
    "        self.proj = nn.Linear(n_embd, n_embd) # projection layer\n",
    "        self.dropout = nn.Dropout(dropout) # dropout layer\n",
    "\n",
    "    def forward(self, x, masks):\n",
    "        out = torch.cat([h(x, masks) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: number of heads\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head # n_embd = head_size * n_head\n",
    "        self.sa = MultiHeadAttention(num_heads=n_head, head_size = head_size) # multi-Head attention component\n",
    "        self.ffwd = FeedFoward(n_embd = n_embd) # simple feed forward neural net component\n",
    "        self.ln1 = nn.LayerNorm(n_embd) # 1st layer normalization component\n",
    "        self.ln2 = nn.LayerNorm(n_embd) # 2nd layer normalization component\n",
    "\n",
    "    # forward pass with skip connections (self-attention followed by computation)\n",
    "    def forward(self, x, masks):\n",
    "        x = x + self.sa(self.ln1(x), masks)\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b1ff1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Projection Layer to find word embeddings and pos embedding\n",
    "        self.embed_proj = nn.Linear(hotvector_dim, n_embd, bias=False)\n",
    "        self.position_embedding = nn.Embedding(block_size, n_embd) # to avoid shape errors\n",
    "        self.blocks = nn.ModuleList([Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        \n",
    "\n",
    "    def forward(self, idx, masks, window_map, targets=None):\n",
    "        B, T, _ = idx.shape\n",
    "\n",
    "        # Process text\n",
    "        pos_embeddings = self.position_embedding(torch.arange(T, device=device).unsqueeze(0))\n",
    "        x_text = self.embed_proj(idx) + pos_embeddings\n",
    "        for block in self.blocks:\n",
    "            x_text = block(x_text, masks) # (B, T, n_embd) # pass x through transformer blocks\n",
    "        x_text = self.ln_f(x_text) # (B, T, n_embd) # pass x through final layer norm\n",
    "\n",
    "        # Aggregate text features based on window_map\n",
    "        unique_window_ids = torch.unique(torch.tensor(window_map))\n",
    "        aggregated_text = []\n",
    "        for window_id in unique_window_ids:\n",
    "            indices = [i for i, w in enumerate(window_map) if w == window_id]\n",
    "            window_outputs = x_text[indices]\n",
    "            aggregated_text.append(window_outputs.mean(dim=0).mean(dim=0))   # Mean pooling for each text\n",
    "\n",
    "        x_text = torch.stack(aggregated_text)  # (Num_Texts, n_embd)\n",
    "        return x_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "631db663",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.image_encoder = CnnModel(feature_dim = 256).to(device)\n",
    "        self.text_encoder = Transformer().to(device)\n",
    "        self.alpha_txt = nn.Parameter(torch.tensor(0.5))\n",
    "        self.alpha_img = nn.Parameter(torch.tensor(0.5))\n",
    "        self.classifier_head = nn.Sequential(\n",
    "           nn.Linear(n_embd, 512), \n",
    "           nn.LeakyReLU(),\n",
    "           nn.Dropout(dropout),\n",
    "           nn.Linear(512, 128),\n",
    "           nn.LeakyReLU(),\n",
    "           nn.Dropout(dropout),\n",
    "           nn.Linear(128, n_classes),\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, idx, images, masks, window_map, targets=None):\n",
    "        # Extract image features\n",
    "        img_out = self.image_encoder(images)\n",
    "        # Extract text features\n",
    "        txt_out = self.text_encoder(idx, masks, window_map)\n",
    "        \n",
    "        # Weighted combination\n",
    "        wt_emb = self.alpha_txt * txt_out + self.alpha_img * img_out\n",
    "\n",
    "        logits = self.classifier_head(wt_emb)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c288f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model\n",
    "@torch.no_grad()\n",
    "def evaluate_model():\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for _ in range(eval_iters):  # Evaluate for a fixed number of iterations\n",
    "        xb, xb_images, yb, masks, window_map = get_batch_with_images('test') # Fetch test batch\n",
    "        logits, _ = model(xb, xb_images, masks, window_map)  # Get model predictions\n",
    "\n",
    "        # Compute predicted classes\n",
    "        predictions = torch.argmax(logits, dim=1)  # Shape: (batch_size,)\n",
    "\n",
    "        # Compare predictions with ground truth\n",
    "        total_correct += (predictions == yb).sum().item()\n",
    "        total_samples += yb.size(0)\n",
    "\n",
    "    # Compute accuracy\n",
    "    accuracy = total_correct / total_samples\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def train_accuracy():\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for _ in range(eval_iters):  # Evaluate for a fixed number of iterations\n",
    "        xb, xb_images, yb, masks, window_map = get_batch_with_images('train') # Fetch train batch\n",
    "        logits, _ = model(xb, xb_images, masks, window_map)  # Get model predictions\n",
    "\n",
    "        # Compute predicted classes\n",
    "        predictions = torch.argmax(logits, dim=1)  # Shape: (batch_size,)\n",
    "\n",
    "        # Compare predictions with ground truth\n",
    "        total_correct += (predictions == yb).sum().item()\n",
    "        total_samples += yb.size(0)\n",
    "\n",
    "    # Compute accuracy\n",
    "    accuracy = total_correct / total_samples\n",
    "    print(f\"Train Accuracy: {accuracy * 100:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def validation_accuracy():\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for _ in range(eval_iters):  # Evaluate for a fixed number of iterations\n",
    "        xb, xb_images, yb, masks, window_map = get_batch_with_images('val') # Fetch train batch\n",
    "        logits, _ = model(xb, xb_images, masks, window_map)  # Get model predictions\n",
    "\n",
    "        # Compute predicted classes\n",
    "        predictions = torch.argmax(logits, dim=1)  # Shape: (batch_size,)\n",
    "\n",
    "        # Compare predictions with ground truth\n",
    "        total_correct += (predictions == yb).sum().item()\n",
    "        total_samples += yb.size(0)\n",
    "\n",
    "    # Compute accuracy\n",
    "    accuracy = total_correct / total_samples\n",
    "#     print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "def test_accuracy():\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for _ in range(eval_iters):  # Evaluate for a fixed number of iterations\n",
    "        xb, xb_images, yb, masks, window_map = get_batch_with_images('test') # Fetch train batch\n",
    "        logits, _ = model(xb, xb_images, masks, window_map)  # Get model predictions\n",
    "\n",
    "        # Compute predicted classes\n",
    "        predictions = torch.argmax(logits, dim=1)  # Shape: (batch_size,)\n",
    "\n",
    "        # Compare predictions with ground truth\n",
    "        total_correct += (predictions == yb).sum().item()\n",
    "        total_samples += yb.size(0)\n",
    "\n",
    "    # Compute accuracy\n",
    "    accuracy = total_correct / total_samples\n",
    "#     print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdd32ac",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.028421 M parameters\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = ClassificationModel()\n",
    "model = model.to(device)\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "best_val_accuracy = 0.0\n",
    "patience = 3\n",
    "cnt = 0\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % (eval_interval) == 0 or iter == max_iters - 1:\n",
    "        losses = {}\n",
    "        model.eval()\n",
    "        for split in ['train', 'val', 'test']:\n",
    "            total_loss = 0.0\n",
    "            for _ in range(eval_iters):\n",
    "                xb, xb_images, yb, masks, window_map = get_batch_with_images(split)\n",
    "                logits, loss = model(xb, xb_images, masks, window_map, yb)\n",
    "                total_loss += loss.item()\n",
    "            avg_loss = total_loss / eval_iters\n",
    "            losses[split] = avg_loss\n",
    "        \n",
    "        val_accuracy = validation_accuracy()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, test loss {losses['test']:.4f}, val accuracy {val_accuracy:.4f}, test accuracy: {test_accuracy()}\")\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            cnt = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth') # saving model weights\n",
    "        else:\n",
    "            cnt += 1\n",
    "        \n",
    "        if cnt >= patience:\n",
    "            print(f\"Early stop at {iter} iterations\")\n",
    "            break\n",
    "        model.train()\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, xb_images, yb, masks, window_map = get_batch_with_images('train')\n",
    "\n",
    "    # Forward pass\n",
    "    logits, loss = model(xb, xb_images, masks, window_map, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model()\n",
    "train_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958ee235",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python (final_2)",
   "language": "python",
   "name": "final_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
